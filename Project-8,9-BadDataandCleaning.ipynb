{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d336805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"SparkDemo\"). \\\n",
    "    config('spark.ui.port','0'). \\\n",
    "    config('spark.shuffle.useOldFethProtocol','true'). \\\n",
    "    config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f566562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>member_id</th><th>total</th></tr>\n",
       "<tr><td>e4c167053d5418230...</td><td>5</td></tr>\n",
       "<tr><td>3f87585a20f702838...</td><td>4</td></tr>\n",
       "<tr><td>76b577467eda5bdbc...</td><td>4</td></tr>\n",
       "<tr><td>ad8e5d384dae17e06...</td><td>4</td></tr>\n",
       "<tr><td>066ddaa64bee66dff...</td><td>3</td></tr>\n",
       "<tr><td>3ae415acd6bbfaac1...</td><td>3</td></tr>\n",
       "<tr><td>d9ce4046daf599732...</td><td>3</td></tr>\n",
       "<tr><td>059d401bb603d9a80...</td><td>3</td></tr>\n",
       "<tr><td>a2356725aa9da92c6...</td><td>3</td></tr>\n",
       "<tr><td>c563428cb58da48ff...</td><td>3</td></tr>\n",
       "<tr><td>5d52e7773cb0efff3...</td><td>3</td></tr>\n",
       "<tr><td>e7d8d16928817ec8f...</td><td>3</td></tr>\n",
       "<tr><td>819453be77718d747...</td><td>3</td></tr>\n",
       "<tr><td>498bb6b1f0099cb47...</td><td>3</td></tr>\n",
       "<tr><td>53789bea7edc660ed...</td><td>3</td></tr>\n",
       "<tr><td>22593a1870543b2db...</td><td>3</td></tr>\n",
       "<tr><td>d3731bad3e6a82515...</td><td>3</td></tr>\n",
       "<tr><td>4ab6205de571ccb7b...</td><td>3</td></tr>\n",
       "<tr><td>c92062bb371842b3d...</td><td>3</td></tr>\n",
       "<tr><td>f54295a60946dedad...</td><td>3</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-----+\n",
       "|           member_id|total|\n",
       "+--------------------+-----+\n",
       "|e4c167053d5418230...|    5|\n",
       "|76b577467eda5bdbc...|    4|\n",
       "|3f87585a20f702838...|    4|\n",
       "|ad8e5d384dae17e06...|    4|\n",
       "|498bb6b1f0099cb47...|    3|\n",
       "|f54295a60946dedad...|    3|\n",
       "|035bf3d8288d803bd...|    3|\n",
       "|53789bea7edc660ed...|    3|\n",
       "|819453be77718d747...|    3|\n",
       "|4ab6205de571ccb7b...|    3|\n",
       "|d3731bad3e6a82515...|    3|\n",
       "|066ddaa64bee66dff...|    3|\n",
       "|c92062bb371842b3d...|    3|\n",
       "|e7d8d16928817ec8f...|    3|\n",
       "|291ca1b09ef11ca3e...|    3|\n",
       "|22593a1870543b2db...|    3|\n",
       "|a2356725aa9da92c6...|    3|\n",
       "|5d52e7773cb0efff3...|    3|\n",
       "|c563428cb58da48ff...|    3|\n",
       "|059d401bb603d9a80...|    3|\n",
       "+--------------------+-----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\" select member_id, count(*) as total from itv009959_lending_club.customers group by member_id order by total desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0125430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>member_id</th><th>emp_title</th><th>emp_length</th><th>home_ownership</th><th>annual_income</th><th>address_state</th><th>address_zipcode</th><th>address_country</th><th>grade</th><th>sub_grade</th><th>verification_status</th><th>total_high_credit_limit</th><th>application_type</th><th>join_annual_income</th><th>verification_status_joint</th><th>ingest_date</th></tr>\n",
       "<tr><td>e4c167053d5418230...</td><td>null</td><td>6</td><td>MORTGAGE</td><td>55000.0</td><td>IL</td><td>604xx</td><td>USA</td><td>B</td><td>B5</td><td>Verified</td><td>207300.0</td><td>Individual</td><td>null</td><td>null</td><td>2024-03-12 17:37:...</td></tr>\n",
       "<tr><td>e4c167053d5418230...</td><td>null</td><td>6</td><td>MORTGAGE</td><td>55000.0</td><td>IL</td><td>604xx</td><td>USA</td><td>B</td><td>B5</td><td>Verified</td><td>171165.0</td><td>Individual</td><td>null</td><td>null</td><td>2024-03-12 17:37:...</td></tr>\n",
       "<tr><td>e4c167053d5418230...</td><td>null</td><td>6</td><td>MORTGAGE</td><td>55000.0</td><td>IL</td><td>604xx</td><td>USA</td><td>B</td><td>B5</td><td>Verified</td><td>138780.0</td><td>Individual</td><td>null</td><td>null</td><td>2024-03-12 17:37:...</td></tr>\n",
       "<tr><td>e4c167053d5418230...</td><td>null</td><td>6</td><td>MORTGAGE</td><td>55000.0</td><td>IL</td><td>604xx</td><td>USA</td><td>B</td><td>B5</td><td>Verified</td><td>129833.0</td><td>Individual</td><td>null</td><td>null</td><td>2024-03-12 17:37:...</td></tr>\n",
       "<tr><td>e4c167053d5418230...</td><td>null</td><td>6</td><td>MORTGAGE</td><td>55000.0</td><td>IL</td><td>604xx</td><td>USA</td><td>B</td><td>B5</td><td>Verified</td><td>110907.0</td><td>Individual</td><td>null</td><td>null</td><td>2024-03-12 17:37:...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
       "|           member_id|emp_title|emp_length|home_ownership|annual_income|address_state|address_zipcode|address_country|grade|sub_grade|verification_status|total_high_credit_limit|application_type|join_annual_income|verification_status_joint|         ingest_date|\n",
       "+--------------------+---------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
       "|e4c167053d5418230...|     null|         6|      MORTGAGE|      55000.0|           IL|          604xx|            USA|    B|       B5|           Verified|               207300.0|      Individual|              null|                     null|2024-03-12 17:37:...|\n",
       "|e4c167053d5418230...|     null|         6|      MORTGAGE|      55000.0|           IL|          604xx|            USA|    B|       B5|           Verified|               171165.0|      Individual|              null|                     null|2024-03-12 17:37:...|\n",
       "|e4c167053d5418230...|     null|         6|      MORTGAGE|      55000.0|           IL|          604xx|            USA|    B|       B5|           Verified|               138780.0|      Individual|              null|                     null|2024-03-12 17:37:...|\n",
       "|e4c167053d5418230...|     null|         6|      MORTGAGE|      55000.0|           IL|          604xx|            USA|    B|       B5|           Verified|               129833.0|      Individual|              null|                     null|2024-03-12 17:37:...|\n",
       "|e4c167053d5418230...|     null|         6|      MORTGAGE|      55000.0|           IL|          604xx|            USA|    B|       B5|           Verified|               110907.0|      Individual|              null|                     null|2024-03-12 17:37:...|\n",
       "+--------------------+---------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\" select * from itv009959_lending_club.customers where member_id like 'e4c167053d5418230%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "441b6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_customers = spark.sql(\" select member_id from (select member_id, count(*) as total from itv009959_lending_club.customers group by member_id having total>1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e522e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>member_id</th></tr>\n",
       "<tr><td>eebbd89aa7efc13e7...</td></tr>\n",
       "<tr><td>5130d0087970e032e...</td></tr>\n",
       "<tr><td>2bae2e4dd6d5f2b21...</td></tr>\n",
       "<tr><td>a2af7506825a7dcff...</td></tr>\n",
       "<tr><td>fc0e468bff11ac7c3...</td></tr>\n",
       "<tr><td>a53e2f488d2d76a30...</td></tr>\n",
       "<tr><td>761b2f1e61999e14e...</td></tr>\n",
       "<tr><td>7115ace193e13d8f3...</td></tr>\n",
       "<tr><td>3b8ffe19f24749a73...</td></tr>\n",
       "<tr><td>4231a55d0199c619a...</td></tr>\n",
       "<tr><td>01b2223757c3b62e7...</td></tr>\n",
       "<tr><td>61be6498c93cadf89...</td></tr>\n",
       "<tr><td>cd1fdca829c443fa7...</td></tr>\n",
       "<tr><td>f284044b881f218c0...</td></tr>\n",
       "<tr><td>f99c6e9cfe3a7a2d2...</td></tr>\n",
       "<tr><td>675151e58a628e87b...</td></tr>\n",
       "<tr><td>d4782ddad8591f44d...</td></tr>\n",
       "<tr><td>cbede54df344cdb94...</td></tr>\n",
       "<tr><td>e884f4108b3c6b1f4...</td></tr>\n",
       "<tr><td>3b94fbab00b7a0ec4...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|           member_id|\n",
       "+--------------------+\n",
       "|3b8ffe19f24749a73...|\n",
       "|cbede54df344cdb94...|\n",
       "|fc0e468bff11ac7c3...|\n",
       "|01b2223757c3b62e7...|\n",
       "|4231a55d0199c619a...|\n",
       "|61be6498c93cadf89...|\n",
       "|675151e58a628e87b...|\n",
       "|7115ace193e13d8f3...|\n",
       "|a2af7506825a7dcff...|\n",
       "|eebbd89aa7efc13e7...|\n",
       "|5130d0087970e032e...|\n",
       "|2bae2e4dd6d5f2b21...|\n",
       "|a53e2f488d2d76a30...|\n",
       "|761b2f1e61999e14e...|\n",
       "|f99c6e9cfe3a7a2d2...|\n",
       "|d4782ddad8591f44d...|\n",
       "|cd1fdca829c443fa7...|\n",
       "|f284044b881f218c0...|\n",
       "|e61af5f79c2480a8b...|\n",
       "|d12c5766068d3b377...|\n",
       "+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62de5ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3157"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data_customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e401fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_loans_defaulters_delinq = spark.sql(\"\"\"\n",
    "select member_id from \n",
    "(select member_id, count(*) as total \n",
    "from itv009959_lending_club.loans_defaulters_delinq \n",
    "group by member_id having total>1) \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f69e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>member_id</th></tr>\n",
       "<tr><td>8d2c605c7ad9209cc...</td></tr>\n",
       "<tr><td>bbe43331566910d55...</td></tr>\n",
       "<tr><td>6918b9861ba5a4c67...</td></tr>\n",
       "<tr><td>b5ded5638e54e1692...</td></tr>\n",
       "<tr><td>a93a44c9d83793451...</td></tr>\n",
       "<tr><td>6c2b63ff231e520d4...</td></tr>\n",
       "<tr><td>694c6cb86608015e9...</td></tr>\n",
       "<tr><td>58dde656f747cee8b...</td></tr>\n",
       "<tr><td>0b44a10bc749eabb2...</td></tr>\n",
       "<tr><td>77db5fdf951dd04b2...</td></tr>\n",
       "<tr><td>62394e3f9d063413b...</td></tr>\n",
       "<tr><td>4f7579700cd9d49d7...</td></tr>\n",
       "<tr><td>17d76ba5141e1c33a...</td></tr>\n",
       "<tr><td>dbe5ec23d01598dbf...</td></tr>\n",
       "<tr><td>c16e1f92ae5ccc785...</td></tr>\n",
       "<tr><td>343344c3c65b023af...</td></tr>\n",
       "<tr><td>db887c1d4cb004ca8...</td></tr>\n",
       "<tr><td>c23eb88fed4893030...</td></tr>\n",
       "<tr><td>ac9a3da3b89f9228e...</td></tr>\n",
       "<tr><td>f1ebb6edb9b07de5f...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|           member_id|\n",
       "+--------------------+\n",
       "|8d2c605c7ad9209cc...|\n",
       "|bbe43331566910d55...|\n",
       "|6918b9861ba5a4c67...|\n",
       "|b5ded5638e54e1692...|\n",
       "|a93a44c9d83793451...|\n",
       "|6c2b63ff231e520d4...|\n",
       "|694c6cb86608015e9...|\n",
       "|58dde656f747cee8b...|\n",
       "|0b44a10bc749eabb2...|\n",
       "|77db5fdf951dd04b2...|\n",
       "|62394e3f9d063413b...|\n",
       "|4f7579700cd9d49d7...|\n",
       "|dbe5ec23d01598dbf...|\n",
       "|17d76ba5141e1c33a...|\n",
       "|c16e1f92ae5ccc785...|\n",
       "|343344c3c65b023af...|\n",
       "|db887c1d4cb004ca8...|\n",
       "|c23eb88fed4893030...|\n",
       "|ac9a3da3b89f9228e...|\n",
       "|f1ebb6edb9b07de5f...|\n",
       "+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data_loans_defaulters_delinq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cea6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_loans_defaulters_detail_rec_enq_df = spark.sql(\"\"\"\n",
    "select member_id from \n",
    "(select member_id, count(*) as total \n",
    "from itv009959_lending_club.loans_defaulters_delinq_detail_records\n",
    "group by member_id having total>1) \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a9d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>member_id</th></tr>\n",
       "<tr><td>cbede54df344cdb94...</td></tr>\n",
       "<tr><td>cd1fdca829c443fa7...</td></tr>\n",
       "<tr><td>761b2f1e61999e14e...</td></tr>\n",
       "<tr><td>3b8ffe19f24749a73...</td></tr>\n",
       "<tr><td>2bae2e4dd6d5f2b21...</td></tr>\n",
       "<tr><td>7115ace193e13d8f3...</td></tr>\n",
       "<tr><td>f284044b881f218c0...</td></tr>\n",
       "<tr><td>eebbd89aa7efc13e7...</td></tr>\n",
       "<tr><td>01b2223757c3b62e7...</td></tr>\n",
       "<tr><td>a53e2f488d2d76a30...</td></tr>\n",
       "<tr><td>fc0e468bff11ac7c3...</td></tr>\n",
       "<tr><td>61be6498c93cadf89...</td></tr>\n",
       "<tr><td>4231a55d0199c619a...</td></tr>\n",
       "<tr><td>675151e58a628e87b...</td></tr>\n",
       "<tr><td>f99c6e9cfe3a7a2d2...</td></tr>\n",
       "<tr><td>a2af7506825a7dcff...</td></tr>\n",
       "<tr><td>d4782ddad8591f44d...</td></tr>\n",
       "<tr><td>5130d0087970e032e...</td></tr>\n",
       "<tr><td>759c2dc5f485e5465...</td></tr>\n",
       "<tr><td>53789bea7edc660ed...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|           member_id|\n",
       "+--------------------+\n",
       "|cbede54df344cdb94...|\n",
       "|cd1fdca829c443fa7...|\n",
       "|761b2f1e61999e14e...|\n",
       "|3b8ffe19f24749a73...|\n",
       "|2bae2e4dd6d5f2b21...|\n",
       "|7115ace193e13d8f3...|\n",
       "|f284044b881f218c0...|\n",
       "|eebbd89aa7efc13e7...|\n",
       "|01b2223757c3b62e7...|\n",
       "|a53e2f488d2d76a30...|\n",
       "|fc0e468bff11ac7c3...|\n",
       "|61be6498c93cadf89...|\n",
       "|4231a55d0199c619a...|\n",
       "|675151e58a628e87b...|\n",
       "|f99c6e9cfe3a7a2d2...|\n",
       "|a2af7506825a7dcff...|\n",
       "|d4782ddad8591f44d...|\n",
       "|5130d0087970e032e...|\n",
       "|22593a1870543b2db...|\n",
       "|46cb438a0bd1b7194...|\n",
       "+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data_loans_defaulters_detail_rec_enq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e22f01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data_loans_defaulters_delinq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b4f0424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3189"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data_loans_defaulters_detail_rec_enq_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ae919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_customers.repartition(1).write \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".mode('overwrite') \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/bad/bad_data_customers_csv\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15dc9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_loans_defaulters_delinq.repartition(1).write \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".mode('overwrite') \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/bad/bad_data_loans_defaulters_delinq_csv\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd2e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_loans_defaulters_detail_rec_enq_df.repartition(1).write \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".mode('overwrite') \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/bad/bad_data_loans_defaulters_detail_rec_enq_csv\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b619b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_customer_consolidated_df = bad_data_customers.select(\"member_id\") \\\n",
    ".union(bad_data_loans_defaulters_delinq.select(\"member_id\")) \\\n",
    ".union(bad_data_loans_defaulters_detail_rec_enq_df.select(\"member_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4975dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_customer_consolidated_df_final = bad_customer_consolidated_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0204e3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3189"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_customer_consolidated_df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48c54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_customer_consolidated_df_final.repartition(1).write \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".mode('overwrite') \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/bad/bad_data_customer_consolidated_finalq_csv\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1315f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_customer_consolidated_df_final.createOrReplaceTempView(\"bad_data_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8007963",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = spark.sql(\"\"\"\n",
    "select * from itv009959_lending_club.customers where \n",
    "member_id not in (\n",
    "select member_id from bad_data_customer\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00f56b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2254192"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92d88852",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.write \\\n",
    ".format(\"parquet\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/cleaned_new/customers_parquet\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05616238",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_defaulters_delinq_df = spark.sql(\"\"\"\n",
    "select * from itv009959_lending_club.loans_defaulters_delinq where \n",
    "member_id not in (\n",
    "select member_id from bad_data_customer\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13715d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_defaulters_delinq_df.write \\\n",
    ".format(\"parquet\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/cleaned_new/loans_defaulters_delinq_parquet\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f61f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_defaulters_delinq_detail_records_df = spark.sql(\"\"\"\n",
    "select * from itv009959_lending_club.loans_defaulters_delinq_detail_records where \n",
    "member_id not in (\n",
    "select member_id from bad_data_customer\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b315f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_defaulters_delinq_detail_records_df.write \\\n",
    ".format(\"parquet\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\",\"/user/itv009959/lendingclubproject/cleaned_new/loans_defaulters_delinq_detail_records_parquet\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b57d934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create EXTERNAL TABLE itv009959_lending_club.customers_new(\n",
    "member_id string, emp_title string, emp_length int,\n",
    "home_ownership string, annual_income float, address_state string, address_zipcode string, \n",
    "address_country string, grade string, sub_grade string, verification_status string, \n",
    "total_high_credit_limit float, application_type string, join_annual_income float, verification_status_joint string, ingest_date timestamp \n",
    ") stored as parquet LOCATION '/user/itv009959/lendingclubproject/cleaned_new/customers_parquet'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36311209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table itv009959_lending_club.customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "417dad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create  EXTERNAL TABLE itv009959_lending_club.loans_defaulters_delinq_new(\n",
    "member_id string, dealing_2yrs int, dealing_amnt float, months_since_last_dealing int\n",
    ") stored as parquet LOCATION '/user/itv009959/lendingclubproject/cleaned_new/loans_defaulters_delinq_parquet'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f7308cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create  EXTERNAL TABLE itv009959_lending_club.loans_defaulters_delinq_detail_records_new(\n",
    "member_id string, pub_rec int, pub_rec_bankruptcies int, inquiry_last_6months int\n",
    ") stored as parquet LOCATION '/user/itv009959/lendingclubproject/cleaned_new/loans_defaulters_delinq_detail_records_parquet'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a84ec2b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o462.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31) \tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628) \tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37) \tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o462.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31) \tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628) \tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37) \tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>member_id</th><th>total</th></tr>\n",
       "<tr><td>7896e26593d77002d...</td><td>1</td></tr>\n",
       "<tr><td>2d812c03838313aa2...</td><td>1</td></tr>\n",
       "<tr><td>8377e79819ae78fa8...</td><td>1</td></tr>\n",
       "<tr><td>f50d2dbad856b169b...</td><td>1</td></tr>\n",
       "<tr><td>2147c673ab2bf6757...</td><td>1</td></tr>\n",
       "<tr><td>aecae2e14dcfc9340...</td><td>1</td></tr>\n",
       "<tr><td>6498c88718e16a462...</td><td>1</td></tr>\n",
       "<tr><td>ffeb15188e88cd928...</td><td>1</td></tr>\n",
       "<tr><td>3791c7cfc042adb79...</td><td>1</td></tr>\n",
       "<tr><td>57ff23ca8b9e7f61e...</td><td>1</td></tr>\n",
       "<tr><td>a52097b094bf2c6b9...</td><td>1</td></tr>\n",
       "<tr><td>9476e0ab92a3a4bc0...</td><td>1</td></tr>\n",
       "<tr><td>910dcf4156322a9ff...</td><td>1</td></tr>\n",
       "<tr><td>dc2562438440c072d...</td><td>1</td></tr>\n",
       "<tr><td>857b4e64307dab673...</td><td>1</td></tr>\n",
       "<tr><td>cc48d4d649a67c92d...</td><td>1</td></tr>\n",
       "<tr><td>30487d7afe32c5b50...</td><td>1</td></tr>\n",
       "<tr><td>2ff7d8dba6cfdbc32...</td><td>1</td></tr>\n",
       "<tr><td>b3bfc9ee24099b36e...</td><td>1</td></tr>\n",
       "<tr><td>125a4ea14068e19ed...</td><td>1</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\" select member_id, count(*) as total from itv009959_lending_club.customers_new group by member_id order by total desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e3c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
